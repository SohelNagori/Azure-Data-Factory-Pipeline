# Azure Data Factory - Copy Data Pipeline

## ğŸ“Œ Overview
This repository contains an **Azure Data Factory (ADF) Copy Data Pipeline** that demonstrates how to copy data from a **source** (e.g., Azure Blob Storage) to a **sink** (e.g., Azure SQL Database).

The pipeline is exported as a JSON file so it can be easily imported into any ADF environment.

---

## ğŸ“‚ Repository Structure
```
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ CopyDataPipeline.json   # ADF pipeline JSON definition
â”œâ”€â”€ datasets/                   # (Optional) Dataset definitions
â”œâ”€â”€ linkedServices/             # (Optional) Linked service definitions
â””â”€â”€ README.md                   # Documentation
```

---

## ğŸš€ Features
- Copies data between supported ADF connectors
- Supports CSV/DelimitedText datasets
- Can be adapted to multiple file formats
- Uses wildcards for multi-file ingestion

---

## ğŸ›  How to Use
### 1ï¸âƒ£ Import into Azure Data Factory
1. Open **Azure Data Factory Studio**.
2. Go to **Author** tab.
3. Click **+ (New pipeline)** â†’ **Import from JSON**.
4. Upload the `CopyDataPipeline.json` file from this repo.

### 2ï¸âƒ£ Configure
- Update **Linked Services** to match your environment (storage account, database, etc.).
- Adjust file paths or wildcards as needed.

### 3ï¸âƒ£ Run the Pipeline
- Debug the pipeline to ensure it runs successfully.
- Publish changes and trigger manually or via schedule.

---

## ğŸ“„ Requirements
- Azure subscription
- Azure Data Factory instance
- Appropriate access to source and sink data stores

---

## ğŸ“Œ Notes
- The JSON file contains only the **pipeline** definition.
- Datasets and Linked Services must be recreated or updated in your environment unless included in the repo.
- For complete export (pipelines, datasets, linked services), use ADF's **Export ARM Template** feature.

---

## ğŸ“§ Contact
If you have any questions or improvements, feel free to open an issue or submit a pull request.
